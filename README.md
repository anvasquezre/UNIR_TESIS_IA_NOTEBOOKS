# Estudio Comparativo de PequeÃ±os Modelos de Lenguaje Afinados (SLMs) y Modelos Grandes de Lenguaje (LLMs) para la ExtracciÃ³n de InformaciÃ³n

## DescripciÃ³n del Proyecto

Este repositorio contiene el cÃ³digo fuente, notebooks y datos para la tesis de maestrÃ­a que realiza un estudio comparativo entre Small Language Models (SLMs) afinados y Large Language Models (LLMs) para tareas de extracciÃ³n de informaciÃ³n en documentos multimodales.

## ğŸ“Š Dataset

El proyecto utiliza el dataset **SP-DocVQA (Single Page Document Visual Question Answering)**, que contiene:
- Documentos de una sola pÃ¡gina con texto e imÃ¡genes
- Preguntas sobre el contenido de los documentos
- Respuestas esperadas para evaluaciÃ³n
- Datos OCR extraÃ­dos de los documentos

## ğŸ—ï¸ Estructura del Proyecto

```
# Notebooks principales de anÃ¡lisis
â”œâ”€â”€ dataset_creation.ipynb     # CreaciÃ³n y preprocesamiento del dataset
â”œâ”€â”€ EDA.ipynb                 # AnÃ¡lisis exploratorio de datos
â”œâ”€â”€ openai-4-mini.ipynb       # Experimentos con GPT-4 Mini
â”œâ”€â”€ claude-3.5-sonnet.ipynb   # Experimentos con Claude 3.5 Sonnet
â”œâ”€â”€ Llama3.2_(11B)-Vision.ipynb # Experimentos con Llama 3.2 Vision
â”œâ”€â”€ results.ipynb             # AnÃ¡lisis de resultados y mÃ©tricas
â”œâ”€â”€ data/                         # Datos procesados
â”‚   â”œâ”€â”€ df_concat_with_text_and_image_tokens.pkl
â”‚   â”œâ”€â”€ train/, val/, test/       # Divisiones del dataset
â”œâ”€â”€ spdocvqa_qas/                 # Datos de preguntas y respuestas
â”œâ”€â”€ spdocvqa_ocr/                 # Datos OCR extraÃ­dos
â”œâ”€â”€ images/                       # ImÃ¡genes del dataset
â”œâ”€â”€ results/                      # Resultados de experimentos por modelo
â”œâ”€â”€ final_results_test/           # Resultados finales en conjunto de prueba
â”œâ”€â”€ models/                       # Definiciones de modelos de base de datos
â”œâ”€â”€ utils/                        # Utilidades y funciones auxiliares
â”œâ”€â”€ core/                         # Configuraciones centrales
â”œâ”€â”€ latex/                        # Archivos LaTeX para tablas de mÃ©tricas
â””â”€â”€ requirements.txt              # Dependencias del proyecto
```

## ğŸ¤– Modelos Evaluados

### Large Language Models (LLMs)
- **GPT-4.1 Mini**: VersiÃ³n mÃ¡s eficiente de GPT-4
- **GPT-4.1 Nano**: VersiÃ³n ultraligera de GPT-4
- **Claude 3.5 Sonnet**: Modelo de Anthropic

### Small Language Models (SLMs)
- **Llama 3.2 (11B) Vision**: Modelo multimodal de Meta
  - Configuraciones evaluadas:
    - Solo texto
    - Solo imagen
    - Multimodal (texto + imagen)
  - Estados de entrenamiento:
    - Modelo base (epoch 0)
    - Fine-tuned epoch 1
    - Fine-tuned epoch 2

## ğŸ“ˆ MÃ©tricas de EvaluaciÃ³n

- **ANLS (Average Normalized Levenshtein Similarity)**: MÃ©trica principal para VQA
- **WER (Word Error Rate)**: Tasa de error a nivel de palabra
- **CER (Character Error Rate)**: Tasa de error a nivel de carÃ¡cter
- **Parsing Error Rate**: Tasa de errores de anÃ¡lisis de respuestas

## ğŸ”§ ConfiguraciÃ³n del Entorno

### Requisitos Previos
- Python 3.8+
- CUDA (para entrenamiento con GPU)
- Docker y Docker Compose (para base de datos)

### InstalaciÃ³n

1. **Clonar el repositorio**
```bash
git clone <url-del-repositorio>
cd TESIS/Notebooks
```

2. **Crear entorno virtual**
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# o
venv\Scripts\activate     # Windows
```

3. **Instalar dependencias**
```bash
pip install -r requirements.txt
```

4. **Configurar base de datos**
```bash
docker-compose up -d
```

5. **Configurar variables de entorno**
```bash
cp .env.example .env
# Editar .env con tus claves de API
```

## ğŸš€ Uso

### 1. PreparaciÃ³n de Datos
```bash
jupyter notebook dataset_creation.ipynb
```
Este notebook:
- Carga los datos de SP-DocVQA
- Extrae texto OCR de las imÃ¡genes
- Calcula tokens para LLMs
- Preprocesa imÃ¡genes para modelos multimodales

### 2. AnÃ¡lisis Exploratorio
```bash
jupyter notebook EDA.ipynb
```
Incluye:
- DistribuciÃ³n de tipos de preguntas
- AnÃ¡lisis de complejidad de documentos
- EstadÃ­sticas de tokens y longitud de texto

### 3. Experimentos con Modelos

#### LLMs (GPT-4, Claude)
```bash
jupyter notebook openai-4-mini.ipynb
jupyter notebook claude-3.5-sonnet.ipynb
```

#### SLMs (Llama)
```bash
jupyter notebook Llama3.2_\(11B\)-Vision.ipynb
```

### 4. AnÃ¡lisis de Resultados
```bash
jupyter notebook results.ipynb
```
Genera:
- Tablas de mÃ©tricas comparativas
- GrÃ¡ficos de rendimiento por epoch
- AnÃ¡lisis por tipo de pregunta
- GrÃ¡ficos radar para comparaciÃ³n multimodal

## ğŸ“Š Resultados Principales

### Rendimiento ANLS por Modelo
| Modelo | Solo Texto | Solo Imagen | Multimodal |
|--------|------------|-------------|------------|
| GPT-4.1 Mini | 0.66 | 0.82 | 0.84 |
| GPT-4.1 Nano | 0.67 | 0.83 | 0.85 |
| Llama 3.2 (Fine-tuned) | 0.36 | 0.84 | 0.86 |

### Hallazgos Clave
- **Llama 3.2 supera a los LLMs en modalidad multimodal**: Con un ANLS de 0.86 vs 0.84-0.85 de GPT-4
- **Rendimiento equivalente en solo imagen**: Llama 3.2 iguala el rendimiento de GPT-4.1 Mini (0.84) y estÃ¡ muy cerca de GPT-4.1 Nano (0.83)
- **Los LLMs mantienen ventaja en solo texto**: GPT-4 supera significativamente a Llama en tareas de solo texto
- **El fine-tuning es crucial para SLMs**: Mejora dramÃ¡tica del rendimiento de Llama 3.2 tras el entrenamiento
- **La modalidad multimodal maximiza el potencial**: Combinando texto e imagen se obtiene el mejor rendimiento
- **Eficiencia vs rendimiento**: Llama 3.2 ofrece rendimiento competitivo o superior con menor costo computacional

## ğŸ› ï¸ Scripts Importantes

- `utils/utils.py`: Funciones de utilidad para procesamiento de texto y tokens
- `core/settings.py`: Configuraciones centrales del proyecto
- `data_utils/docvqa_imdbs_data.py`: Utilidades especÃ­ficas para DocVQA
- `training/`: Scripts de fine-tuning para Llama

## ğŸ“ Estructura de Datos

### Dataset Principal
- **Entrenamiento**: ~40.000 ejemplos
- **ValidaciÃ³n**: ~5000 ejemplos  
- **Prueba**: ~5000 ejemplos

### Tipos de Preguntas
- Layout: Preguntas sobre estructura del documento
- Table: Preguntas sobre contenido tabular
- List: Preguntas sobre listas e Ã­tems
- Form: Preguntas sobre formularios
- Figure: Preguntas sobre grÃ¡ficos e imÃ¡genes
- FreeText: Preguntas abiertas sobre el contenido
- Yes/No: Preguntas de respuesta binaria

## ğŸ” MetodologÃ­a

1. **Preprocesamiento**: ExtracciÃ³n OCR, tokenizaciÃ³n, redimensionamiento de imÃ¡genes
2. **ExperimentaciÃ³n**: EvaluaciÃ³n sistemÃ¡tica de todos los modelos
3. **Fine-tuning**: Entrenamiento de Llama 3.2 con LoRA
4. **EvaluaciÃ³n**: MÃ©tricas estÃ¡ndar de VQA y anÃ¡lisis por categorÃ­as
5. **AnÃ¡lisis**: ComparaciÃ³n estadÃ­stica y visualizaciÃ³n de resultados

## ğŸ“‹ Base de Datos

El proyecto utiliza PostgreSQL para almacenar:
- Metadatos de documentos
- Preguntas y respuestas
- Resultados de experimentos
- MÃ©tricas calculadas

Estructura principal:
- `documents`: InformaciÃ³n de documentos y OCR
- `questions`: Preguntas y respuestas del dataset
- `results`: Resultados de predicciones por modelo

## ğŸ¤ ContribuciÃ³n

Este es un proyecto de tesis acadÃ©mica. Para consultas o colaboraciones:
- Revisar la documentaciÃ³n en los notebooks
- Consultar los resultados en `final_results_test/`
- Ver anÃ¡lisis detallados en `results.ipynb`

## ğŸ“š Referencias

- SP-DocVQA Dataset: [SP-DocVQA](https://www.docvqa.org/)
- Llama 3.2 Vision: Meta AI
- GPT-4 Mini/Nano: OpenAI
- ANLS Metric: 

```
@inproceedings{inproceedings,
author = {Bai, Haoli and Liu, Zhiguang and Meng, Xiaojun and Wentao, Li and Liu, Shuang and Luo, Yifeng and Xie, Nian and Zheng, Rongfu and Wang, Liangwei and Hou, Lu and Wei, Jiansheng and Jiang, Xin and Liu, Qun},
year = {2023},
month = {01},
pages = {13386-13401},
title = {Wukong-Reader: Multi-modal Pre-training for Fine-grained Visual Document Understanding},
doi = {10.18653/v1/2023.acl-long.748}
}
```
## ğŸ“„ Licencia

Este proyecto es parte de una tesis acadÃ©mica de la Universidad Internacional de La Rioja (UNIR).

---

**Autor**: Andres Vasquez Restrepo
**Programa**: MaestrÃ­a en Inteligencia Artificial
**Universidad**: Universidad Internacional de La Rioja (UNIR)  
**AÃ±o**: 2025
